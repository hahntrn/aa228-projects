{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import loggamma\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_gradient = lambda d, L2_max: min(L2_max / np.linalg.norm(d), 1) * d\n",
    "# class GradientQLearning:\n",
    "#     def __init__(self, n_actions, discount, lr):\n",
    "#         self.n_actions = n_actions\n",
    "#         self.action_space = [0 for _ in range(n_actions)]\n",
    "#         self.discount_rate = discount # gamma\n",
    "#         self.Q = Q # action value function Q(theta, s, a)\n",
    "#         self.dQ = None\n",
    "#         self.theta = None # action value function parameter\n",
    "#         self.lr = lr # alpha: learning rate\n",
    "\n",
    "# def lookahead(model, s, a):\n",
    "#     return model.Q(model.theta, s, a)\n",
    "\n",
    "# def update(m, s, a, r, sp):\n",
    "#     u = max(m.Q(m.theta, sp, ap) for ap in m.action_space)\n",
    "#     d = (r + m.discount_rate*u - m.Q(m.theta, s, a)) * m.dQ(m.theta, s, a)\n",
    "#     m.theta += m.lr * scale_gradient(d, 1)\n",
    "#     return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MDP:\n",
    "#     def __init__(self, n_states, n_actions, discount, lr):\n",
    "#         self.n_states = n_states\n",
    "#         self.n_actions = n_actions\n",
    "#         self.state_space = np.zeros((n_states)) # [0 for _ in range(n_states)]\n",
    "#         self.action_space = np.zeros((n_actions)) # [0 for _ in range(n_actions)]\n",
    "#         self.discount_rate = discount # gamma\n",
    "#         self.reward_func = None\n",
    "#         self.transition_reward = None\n",
    " \n",
    "\n",
    "# def simulate(mdp, model, policy, horizon, s):\n",
    "#     for _ in range(horizon):\n",
    "#         a = policy(model, s)\n",
    "#         sp, r = mdp.tr(s,a)\n",
    "#         update(model, s, a, r, sp)\n",
    "#         s = sp\n",
    "\n",
    "# class PolicyIteration:\n",
    "#     def __init__(self, k_max, n_states, n_actions):\n",
    "#         self.policy = np.random.randint(0, n_actions, n_states) # initialize random actions in each state\n",
    "#         self.k_max = k_max\n",
    "    \n",
    "# def policy_evaluation(mdp, policy):\n",
    "#     rewardsp = [mdp.reward_func(s, policy[s]) for s in mdp.state_space]\n",
    "#     transitionp = [mdp]\n",
    "    \n",
    "# def solve(model, mdp):\n",
    "#     for _ in range(model.k_max):\n",
    "#         utility = policy_evaluation(mdp, model.policy)\n",
    "#         policyp = ValueFunctionPolicy(mdp, utility)\n",
    "#         if np.all(model.policy[s] == policyp[s] for s in mdp.state_space):\n",
    "#             break\n",
    "#         model.policy = policyp\n",
    "#     return model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearning:\n",
    "    def __init__(self, n_states, n_actions, discount, lr, initial_Q_value):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.state_space = np.zeros((n_states)) # [0 for _ in range(n_states)]\n",
    "        self.action_space = np.zeros((n_actions)) # [0 for _ in range(n_actions)]\n",
    "        self.discount_rate = discount # gamma\n",
    "        self.Q = np.zeros((n_states, n_actions)) # action value function Q(s, a)\n",
    "        self.Q.fill(initial_Q_value) \n",
    "        self.lr = lr # alpha: learning rate\n",
    "\n",
    "lookahead = lambda m, s, a: m.Q[s, a]\n",
    "def update(m, s, a, r, sp):\n",
    "    if m.Q[s,a] == -np.inf:\n",
    "        m.Q[s,a] = 0.\n",
    "    m.Q[s, a] += m.lr * (r + m.discount_rate * np.max(m.Q[sp]) - m.Q[s,a])\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21960 49252\n",
      "(50000, 7)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "dataset = 'medium'\n",
    "df = pd.read_csv(f'data/{dataset}.csv')\n",
    "print(df.nunique()['s'], df['s'].max())\n",
    "df['s'] -= 1\n",
    "df['a'] -= 1\n",
    "df['sp'] -= 1\n",
    "# counts = df.nunique()\n",
    "if dataset == 'medium':\n",
    "    model = QLearning(50000, df['a'].max()+1, 1., 0.01, -np.inf)\n",
    "else:\n",
    "    model = QLearning(df['s'].max()+1, df['a'].max()+1, 0.95, 0.01, -np.inf)\n",
    "print(model.Q.shape)\n",
    "for i, obs in df.iterrows():\n",
    "    model = update(model, obs['s'], obs['a'], obs['r'], obs['sp'])\n",
    "policy = np.argmax(model.Q, axis=1) + 1\n",
    "print(np.all(policy==1.0))\n",
    "np.savetxt(f'data/{dataset}.policy', policy.astype(int), fmt='%i')\n",
    "\n",
    "# epsilon greedy 15.9 to call update from simulate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87a02f55084969d82743a2c90d54a08761f8a6d63c430db74c713ccba330ebec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
